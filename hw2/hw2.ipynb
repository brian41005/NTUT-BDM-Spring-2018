{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 103590450 四資四 馬茂源"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](1.png)\n",
    "![](2.png)\n",
    "![](3.png)\n",
    "![](4.png)\n",
    "![](5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:43:12.758897Z",
     "start_time": "2018-04-12T17:43:12.751377Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.sql.functions import udf, mean\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, math, time\n",
    "import itertools\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:43:12.766417Z",
     "start_time": "2018-04-12T17:43:12.759900Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('result'):\n",
    "    os.mkdir('result')\n",
    "for i in range(1, 5):\n",
    "    dir_ = 'result/task{}'.format(i)\n",
    "    if not os.path.exists(dir_):\n",
    "        os.mkdir(dir_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:43:12.772433Z",
     "start_time": "2018-04-12T17:43:12.767420Z"
    }
   },
   "outputs": [],
   "source": [
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:43:12.780454Z",
     "start_time": "2018-04-12T17:43:12.773436Z"
    }
   },
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "        #.setMaster('spark://10.100.5.182:7077')\n",
    "        #.setMaster(\"local\")\n",
    "        .setAppName(\"hw2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:43:12.786972Z",
     "start_time": "2018-04-12T17:43:12.781959Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sc = SparkContext(conf=conf)\n",
    "    sql_sc = SQLContext(sc)\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:43:12.793489Z",
     "start_time": "2018-04-12T17:43:12.787974Z"
    }
   },
   "outputs": [],
   "source": [
    "files = {'fb':['Facebook_Economy.csv', \n",
    "               'Facebook_Obama.csv', \n",
    "               'Facebook_Palestine.csv', \n",
    "               'Facebook_Microsoft.csv'],\n",
    "        'google':['GooglePlus_Obama.csv', \n",
    "                  'GooglePlus_Palestine.csv', \n",
    "                  'GooglePlus_Economy.csv', \n",
    "                  'GooglePlus_Microsoft.csv'],\n",
    "        'linkedin':['LinkedIn_Microsoft.csv', \n",
    "                    'LinkedIn_Palestine.csv',\n",
    "                    'LinkedIn_Obama.csv', \n",
    "                    'LinkedIn_Economy.csv'],\n",
    "        'news':'News_Final.csv'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* IDLink (numeric): Unique identifier of news items\n",
    "* Title (string): Title of the news item according to the official media sources\n",
    "* Headline (string): Headline of the news item according to the official media sources\n",
    "* Source (string): Original news outlet that published the news item\n",
    "* Topic (string): Query topic used to obtain the items in the official media sources\n",
    "* PublishDate (timestamp): Date and time of the news items' publication\n",
    "* SentimentTitle (numeric): Sentiment score of the text in the news items' title\n",
    "* SentimentHeadline (numeric): Sentiment score of the text in the news items' headline\n",
    "* Facebook (numeric): Final value of the news items' popularity according to the social media source Facebook\n",
    "* GooglePlus (numeric): Final value of the news items' popularity according to the social media source Google+\n",
    "* LinkedIn (numeric): Final value of the news items' popularity according to the social media source LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:43:12.800006Z",
     "start_time": "2018-04-12T17:43:12.794492Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_csv(file_name):\n",
    "    try:\n",
    "        data = sql_sc.read.csv(file_name, \n",
    "                       sep=',', \n",
    "                       header=True, \n",
    "                       mode='DROPMALFORMED')\n",
    "    except AnalysisException:\n",
    "        data = sql_sc.read.csv('hdfs:///bdm/hw2/{}'.format(file_name), \n",
    "                       sep=',', \n",
    "                       header=True, \n",
    "                       mode='DROPMALFORMED')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:43:12.935867Z",
     "start_time": "2018-04-12T17:43:12.801510Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "news = read_csv(files['news'])\n",
    "# news = (sql_sc.read.load(files['news'], \n",
    "#                          format=\"csv\", \n",
    "#                          schema=StructType([StructField(\"IDLink\", StringType(), False),\n",
    "#                                             StructField(\"Title\", StringType(), False),\n",
    "#                                             StructField(\"Headline\", StringType(), False),\n",
    "#                                             StructField(\"Source\", StringType(), False),\n",
    "#                                             StructField(\"Topic\", StringType(), False),\n",
    "#                                             StructField(\"PublishDate\", StringType(), False),\n",
    "#                                             StructField(\"SentimentTitle\", StringType(), False),\n",
    "#                                             StructField(\"SentimentHeadline\", StringType(), False),\n",
    "#                                             StructField(\"Facebook\", StringType(), False),\n",
    "#                                             StructField(\"GooglePlus\", StringType(), False),\n",
    "#                                             StructField(\"LinkedIn\", StringType(), False)]),\n",
    "#                          mode=\"DROPMALFORMED\", \n",
    "#                          header=\"true\")\n",
    "#         .drop('IDLink')\n",
    "#         .drop('Source')\n",
    "#         .drop('SentimentTitle')\n",
    "#         .drop('SentimentHeadline')\n",
    "#         .drop('Facebook')\n",
    "#         .drop('GooglePlus')\n",
    "#         .drop('LinkedIn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:43:12.938875Z",
     "start_time": "2018-04-12T17:43:12.936870Z"
    }
   },
   "outputs": [],
   "source": [
    "# news = news.sample(False, 0.01, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:43:12.950406Z",
     "start_time": "2018-04-12T17:43:12.939878Z"
    }
   },
   "outputs": [],
   "source": [
    "news = news.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:43:12.962939Z",
     "start_time": "2018-04-12T17:43:12.951910Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_data = news.select('title', \n",
    "                        'headline', \n",
    "                        'topic', \n",
    "                        'publishDate',\n",
    "                        'SentimentTitle', \n",
    "                        'SentimentHeadline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:43:12.967451Z",
     "start_time": "2018-04-12T17:43:12.963942Z"
    }
   },
   "outputs": [],
   "source": [
    "def wordTokenizer(data, columns):\n",
    "    for c in columns:\n",
    "        new_c = c + '_tokens'\n",
    "        reTokenizer = RegexTokenizer(inputCol=c, \n",
    "                                     outputCol=new_c, \n",
    "                                     pattern='\\\\W', \n",
    "                                     minTokenLength=2)\n",
    "        data = reTokenizer.transform(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:43:12.994022Z",
     "start_time": "2018-04-12T17:43:12.968454Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "col =  ['title', 'headline']\n",
    "news_data = wordTokenizer(news_data, col)\n",
    "news_data = news_data.select('title_tokens', \n",
    "                             'headline_tokens', \n",
    "                             'topic',  \n",
    "                             'publishDate',\n",
    "                             'SentimentTitle', \n",
    "                             'SentimentHeadline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:43:13.007056Z",
     "start_time": "2018-04-12T17:43:12.995526Z"
    }
   },
   "outputs": [],
   "source": [
    "news_data = news_data.withColumn('publishDate', \n",
    "                                 udf(lambda tmp: tmp[:10] , StringType())\n",
    "                                 (news_data.publishDate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:43:13.026609Z",
     "start_time": "2018-04-12T17:43:13.008059Z"
    }
   },
   "outputs": [],
   "source": [
    "news_data = news_data.withColumn('SentimentScore', (news_data.SentimentTitle+news_data.SentimentHeadline)/2)\n",
    "news_data = news_data.select('title_tokens', \n",
    "                             'headline_tokens', \n",
    "                             'topic',  \n",
    "                             'publishDate',\n",
    "                             'SentimentScore')                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:43:13.835759Z",
     "start_time": "2018-04-12T17:43:13.027612Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+-----------+--------------------+\n",
      "|        title_tokens|     headline_tokens|    topic|publishDate|      SentimentScore|\n",
      "+--------------------+--------------------+---------+-----------+--------------------+\n",
      "|[obama, lays, wre...|[obama, lays, wre...|    obama| 2002-04-02| -0.0266500895444513|\n",
      "|[look, at, the, h...|[tim, haywood, in...|  economy| 2008-09-20|  0.0259737613952635|\n",
      "|[nouriel, roubini...|[nouriel, roubini...|  economy| 2012-01-28|  -0.142727891770822|\n",
      "|[finland, gdp, ex...|[finland, economy...|  economy| 2015-03-01| 0.01303215087856715|\n",
      "|[tourism, govt, s...|[tourism, and, pu...|  economy| 2015-03-01|  0.0705422282441575|\n",
      "|[intellitec, solu...|[over, 100, atten...|microsoft| 2015-03-01|-0.01930252131229...|\n",
      "|[obama, stars, pa...|[first, lady, mic...|    obama| 2015-03-01| 0.09316804504927514|\n",
      "|[fire, claims, mo...|[hancock, county,...|palestine| 2015-03-01|-0.11205498154672319|\n",
      "|[microsoft, new, ...|[new, delhi, feb,...|microsoft| 2015-03-01|-0.07062563266677825|\n",
      "|[microsoft, proje...|[microsoft, may, ...|microsoft| 2015-03-01| 0.00127497041897463|\n",
      "|[microsoft, sneak...|[the, platform, b...|microsoft| 2015-03-01| 0.02633513150296405|\n",
      "|[greek, economy, ...|[greece, economy,...|  economy| 2015-03-01|  -0.187629664616396|\n",
      "|[big, data, and, ...|[big, data, analy...|  economy| 2015-03-01| 0.05109553912555111|\n",
      "|[hololens, dev, e...|[microsoft, ar, h...|microsoft| 2015-03-01| 0.03971684226033435|\n",
      "|[microsoft, word,...|[what, is, welcom...|microsoft| 2015-03-01|-0.02807912926000375|\n",
      "|[microsoft, band,...|[the, microsoft, ...|microsoft| 2015-03-01|-0.03543416934461505|\n",
      "|[microsoft, prepa...|[it, seems, that,...|microsoft| 2015-03-01|        6.8359375E-4|\n",
      "|[greek, economy, ...|[greece, economy,...|  economy| 2015-03-01| -0.0925881457513656|\n",
      "|[sweden, economy,...|[sweden, economy,...|  economy| 2015-03-01|  -0.007086833868923|\n",
      "|[the, microsoft, ...|[the, microsoft, ...|microsoft| 2015-03-01|0.021093190275797053|\n",
      "+--------------------+--------------------+---------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_data = news_data.dropna()\n",
    "news_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In news data, count the words in two fields: ‘Title’ and ‘Headline’ respectively, and list the most frequent words according to the term frequency in descending order, in total, per day, and per topic, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:43:13.842277Z",
     "start_time": "2018-04-12T17:43:13.837265Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def word_count_total(data, column, n=10):\n",
    "    return (news_data.select(column)\n",
    "            .rdd\n",
    "            .flatMap(lambda tokens: tokens[column])\n",
    "            .map(lambda word: (word, 1))\n",
    "            .reduceByKey(lambda a, b: a + b)\n",
    "            .sortBy(lambda w: w[1], ascending=False)\n",
    "            .take(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:43:13.851802Z",
     "start_time": "2018-04-12T17:43:13.843781Z"
    }
   },
   "outputs": [],
   "source": [
    "task1_file = open('result/task1/output.txt', 'w', encoding='utf-8', newline='\\n')\n",
    "task1_output = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:43:28.580450Z",
     "start_time": "2018-04-12T17:43:13.853306Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "task1_output.append('[title top-frequent words in total]')\n",
    "for r in word_count_total(news_data, 'title_tokens', n=10):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:43:43.140650Z",
     "start_time": "2018-04-12T17:43:28.582957Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "task1_output.append('[headline top-frequent words in total]')\n",
    "for r in word_count_total(news_data, 'headline_tokens', n=10):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T03:53:33.945638Z",
     "start_time": "2018-04-12T03:53:29.739960Z"
    },
    "scrolled": true
   },
   "source": [
    " #### per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:43:43.162709Z",
     "start_time": "2018-04-12T17:43:43.143156Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_count_per(data, column, per_col, take=-1):\n",
    "    rdd = (news_data.select(column, per_col)\n",
    "            .rdd\n",
    "            .flatMap(lambda row: [((row[per_col], w), 1) for w in row[column]])\n",
    "            .reduceByKey(lambda a, b: a + b)\n",
    "            .map(lambda pair: (pair[0][0], (pair[0][1], pair[1])))\n",
    "            .reduceByKey(lambda a, b: a if a[1] > b[1] else b)\n",
    "            .sortBy(lambda w: w[1][1], ascending=False)\n",
    "            )\n",
    "    if take == -1:\n",
    "        return rdd.collect()\n",
    "    else:\n",
    "        return rdd.take(take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:44:04.301393Z",
     "start_time": "2018-04-12T17:43:43.166219Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task1_output.append('[title top-frequent words per day]')\n",
    "for r in word_count_per(news_data, 'title_tokens', 'publishDate', take=60):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:44:25.919351Z",
     "start_time": "2018-04-12T17:44:04.303900Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task1_output.append('[headline top-frequent words per day]')\n",
    "for r in word_count_per(news_data, 'headline_tokens', 'publishDate', take=60):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:44:44.810061Z",
     "start_time": "2018-04-12T17:44:25.920354Z"
    }
   },
   "outputs": [],
   "source": [
    "task1_output.append('[title top-frequent words per topic]')\n",
    "for r in word_count_per(news_data, 'title_tokens', 'topic'):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:04.090307Z",
     "start_time": "2018-04-12T17:44:44.812067Z"
    }
   },
   "outputs": [],
   "source": [
    "task1_output.append('[headline top-frequent words per topic]')\n",
    "for r in word_count_per(news_data, 'headline_tokens', 'topic'):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:04.104345Z",
     "start_time": "2018-04-12T17:45:04.092814Z"
    }
   },
   "outputs": [],
   "source": [
    "task1_file.writelines(['{}\\n'.format(r) for r in task1_output])\n",
    "task1_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In social feedback data, calculate the average popularity of each news by hour, and by day, respectively (for each platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:04.113870Z",
     "start_time": "2018-04-12T17:45:04.107855Z"
    }
   },
   "outputs": [],
   "source": [
    "fb_social_data = google_social_data = linkedin_social_data = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:04.123897Z",
     "start_time": "2018-04-12T17:45:04.115875Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_social_data(data, files):\n",
    "    for f in files:\n",
    "        df = read_csv(f)\n",
    "        data = data.union(df) if data else df\n",
    "    for i in range(1, 144+1):\n",
    "        col_name = 'TS{}'.format(i)\n",
    "        data = data.withColumn(col_name, data[col_name].cast('int'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:07.817212Z",
     "start_time": "2018-04-12T17:45:04.126404Z"
    }
   },
   "outputs": [],
   "source": [
    "fb_social_data = create_social_data(fb_social_data, files['fb'])\n",
    "google_social_data = create_social_data(google_social_data, files['google'])\n",
    "linkedin_social_data = create_social_data(linkedin_social_data, files['linkedin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:07.903441Z",
     "start_time": "2018-04-12T17:45:07.818215Z"
    }
   },
   "outputs": [],
   "source": [
    "fb_social_data = fb_social_data.dropna()\n",
    "google_social_data = google_social_data.dropna()\n",
    "linkedin_social_data = linkedin_social_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:07.906450Z",
     "start_time": "2018-04-12T17:45:07.904444Z"
    }
   },
   "outputs": [],
   "source": [
    "hour = 3\n",
    "day = 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:07.913468Z",
     "start_time": "2018-04-12T17:45:07.907453Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_avg(seq):\n",
    "    sum_ = np.sum(seq)\n",
    "    return sum_/48, sum_/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:07.919985Z",
     "start_time": "2018-04-12T17:45:07.914471Z"
    }
   },
   "outputs": [],
   "source": [
    "def avg_popu(data, by=3):\n",
    "#     return (data\n",
    "#             .rdd\n",
    "#             .map(lambda r: (r['IDLink'], \n",
    "#                                  [np.mean(chunk) for chunk in zip(*[iter(r[1:])]*by)]))\n",
    "#             .collect())\n",
    "    return (data\n",
    "           .rdd\n",
    "           .map(lambda r: (r['IDLink'],  get_avg(r[1:])))\n",
    "           .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:12.501162Z",
     "start_time": "2018-04-12T17:45:07.920989Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fb_avg_by_hour_and_day = avg_popu(fb_social_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:16.980066Z",
     "start_time": "2018-04-12T17:45:12.502165Z"
    }
   },
   "outputs": [],
   "source": [
    "google_avg_by_hour_and_day = avg_popu(google_social_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:21.487046Z",
     "start_time": "2018-04-12T17:45:16.981069Z"
    }
   },
   "outputs": [],
   "source": [
    "linkedin_avg_by_hour_and_day = avg_popu(linkedin_social_data, by=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:21.505094Z",
     "start_time": "2018-04-12T17:45:21.488048Z"
    }
   },
   "outputs": [],
   "source": [
    "avg_popularity = {'fb':fb_avg_by_hour_and_day,\n",
    "                 'google':google_avg_by_hour_and_day,\n",
    "                 'linkedin':linkedin_avg_by_hour_and_day}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:21.513616Z",
     "start_time": "2018-04-12T17:45:21.506096Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_csv(file_name, data):\n",
    "    with open(file_name, 'w', \n",
    "              encoding='utf-8', newline='\\n') as csvfile:\n",
    "        fieldnames = ['IDLink', 'avg_popularity']\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        writer.writerow(fieldnames)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:21.990384Z",
     "start_time": "2018-04-12T17:45:21.514619Z"
    }
   },
   "outputs": [],
   "source": [
    "for platform, data in avg_popularity.items():\n",
    "    rows_by_hour = []\n",
    "    rows_by_day = []\n",
    "    \n",
    "    for ID, (avg_by_hour, avg_by_day) in data:\n",
    "        rows_by_hour.append((ID, avg_by_hour))\n",
    "        rows_by_day.append( (ID, avg_by_day))\n",
    "        \n",
    "    save_csv('./result/task2/{}_avg_popularity_by_hour.csv'.format(platform), \n",
    "             rows_by_hour)\n",
    "    save_csv('./result/task2/{}_avg_popularity_by_day.csv'.format(platform), \n",
    "             rows_by_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  In news data, calculate the sum and average sentiment score of each topic, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:21.994394Z",
     "start_time": "2018-04-12T17:45:21.991386Z"
    }
   },
   "outputs": [],
   "source": [
    "task3_file = open('result/task3/output.txt', 'w', encoding='utf-8', newline='\\n')\n",
    "task3_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:29.854788Z",
     "start_time": "2018-04-12T17:45:21.995397Z"
    }
   },
   "outputs": [],
   "source": [
    "sum_of_score = (news_data.select('SentimentScore', 'topic')\n",
    "               .rdd\n",
    "               .map(lambda r: (r['topic'], r['SentimentScore']))\n",
    "               .reduceByKey(lambda a, b: a + b)\n",
    "               .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:29.861806Z",
     "start_time": "2018-04-12T17:45:29.856792Z"
    }
   },
   "outputs": [],
   "source": [
    "task3_output.append('[sum sentiment score of each topic]')\n",
    "for topic_row in sum_of_score:\n",
    "    task3_output.append('{:>10s}, {:.3f}'.format(*topic_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:35.428601Z",
     "start_time": "2018-04-12T17:45:29.865316Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count = news_data.count()\n",
    "avg_of_score = (sc.parallelize(sum_of_score)\n",
    "               .map(lambda s: (s[0], s[1] / count))\n",
    "               .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:35.435622Z",
     "start_time": "2018-04-12T17:45:35.431109Z"
    }
   },
   "outputs": [],
   "source": [
    "task3_output.append('[avg sentiment score of each topic]')\n",
    "for topic_row in avg_of_score:\n",
    "    task3_output.append('{:>10s}, {:.6f}'.format(*topic_row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From subtask (1), for the top-100 frequent words per topic in titles and headlines, calculate their co-occurrence matrices (100x100), respectively. Each entry in the matrix will contain the co-occurrence frequency in all news titles and headlines, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:35.456175Z",
     "start_time": "2018-04-12T17:45:35.438630Z"
    }
   },
   "outputs": [],
   "source": [
    "def top_100(data, column, per_col):\n",
    "    return (news_data.select(column, per_col)\n",
    "            .rdd\n",
    "            .flatMap(lambda row: [((row[per_col], w), 1) for w in row[column]])\n",
    "            .reduceByKey(lambda a, b: a + b)\n",
    "            .map(lambda pair: (pair[0][0], (pair[0][1], pair[1])))\n",
    "            .groupByKey()\n",
    "            .map(lambda topic: (topic[0], sorted(topic[1], \n",
    "                                                 key=lambda x: x[1], \n",
    "                                                 reverse=True)[:100]))\n",
    "            .map(lambda topic: (topic[0], [w[0] for w in topic[1]]))\n",
    "            .collect()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:56.865078Z",
     "start_time": "2018-04-12T17:45:35.458682Z"
    }
   },
   "outputs": [],
   "source": [
    "fw_all = {'title_tokens':dict(top_100(news_data, 'title_tokens', 'topic')), \n",
    "         'headline_tokens':dict(top_100(news_data, 'headline_tokens', 'topic'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:45:56.871596Z",
     "start_time": "2018-04-12T17:45:56.867083Z"
    }
   },
   "outputs": [],
   "source": [
    "def counter(vocabulary, tokens):\n",
    "    return  [int(tokens.count(v) > 0) for v in vocabulary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:46:52.765657Z",
     "start_time": "2018-04-12T17:45:56.875105Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col_name, v in fw_all.items():\n",
    "    for topic, vocabulary in v.items():\n",
    "        #print('column name:{}, topic:{}'.format(col_name, topic))\n",
    "        \n",
    "        X = np.array(news_data.select(col_name, 'topic')\n",
    "                     .rdd\n",
    "                     .filter(lambda r: r['topic'] == topic)\n",
    "                     .map(lambda r:counter(vocabulary, r[col_name]))\n",
    "                     .collect(), dtype='int64')\n",
    "        co_occ = X.T.dot(X)\n",
    "        np.fill_diagonal(co_occ, 0)\n",
    "        (pd.DataFrame(data=co_occ, columns=vocabulary, index=vocabulary)\n",
    "         .to_csv('result/task4/{}_{}_matrix.csv'.format(col_name, topic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T17:46:52.769167Z",
     "start_time": "2018-04-12T17:46:52.766659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost 3.67 minutes\n"
     ]
    }
   ],
   "source": [
    "print('cost {:.2f} minutes'.format((time.time()-t0)/60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "171px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
