{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 103590450 四資四 馬茂源"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](1.png)\n",
    "![](2.png)\n",
    "![](3.png)\n",
    "![](4.png)\n",
    "![](5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:18.005551Z",
     "start_time": "2018-04-13T06:13:17.239014Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.sql.functions import udf, mean\n",
    "from pyspark.sql.types import StringType, StructType, StructField, FloatType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, math, time\n",
    "import itertools\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:18.013728Z",
     "start_time": "2018-04-13T06:13:18.008470Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('result'):\n",
    "    os.mkdir('result')\n",
    "for i in range(1, 5):\n",
    "    dir_ = 'result/task{}'.format(i)\n",
    "    if not os.path.exists(dir_):\n",
    "        os.mkdir(dir_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:18.018823Z",
     "start_time": "2018-04-13T06:13:18.016009Z"
    }
   },
   "outputs": [],
   "source": [
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:18.026902Z",
     "start_time": "2018-04-13T06:13:18.021357Z"
    }
   },
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "        #.setMaster('spark://10.100.5.182:7077')\n",
    "        #.setMaster(\"local\")\n",
    "        .setAppName(\"hw2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:21.049009Z",
     "start_time": "2018-04-13T06:13:18.029234Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sc = SparkContext(conf=conf)\n",
    "    sql_sc = SQLContext(sc)\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:21.059861Z",
     "start_time": "2018-04-13T06:13:21.051623Z"
    }
   },
   "outputs": [],
   "source": [
    "files = {'fb':['Facebook_Economy.csv', \n",
    "               'Facebook_Obama.csv', \n",
    "               'Facebook_Palestine.csv', \n",
    "               'Facebook_Microsoft.csv'],\n",
    "        'google':['GooglePlus_Obama.csv', \n",
    "                  'GooglePlus_Palestine.csv', \n",
    "                  'GooglePlus_Economy.csv', \n",
    "                  'GooglePlus_Microsoft.csv'],\n",
    "        'linkedin':['LinkedIn_Microsoft.csv', \n",
    "                    'LinkedIn_Palestine.csv',\n",
    "                    'LinkedIn_Obama.csv', \n",
    "                    'LinkedIn_Economy.csv'],\n",
    "        'news':'News_Final.csv'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* IDLink (numeric): Unique identifier of news items\n",
    "* Title (string): Title of the news item according to the official media sources\n",
    "* Headline (string): Headline of the news item according to the official media sources\n",
    "* Source (string): Original news outlet that published the news item\n",
    "* Topic (string): Query topic used to obtain the items in the official media sources\n",
    "* PublishDate (timestamp): Date and time of the news items' publication\n",
    "* SentimentTitle (numeric): Sentiment score of the text in the news items' title\n",
    "* SentimentHeadline (numeric): Sentiment score of the text in the news items' headline\n",
    "* Facebook (numeric): Final value of the news items' popularity according to the social media source Facebook\n",
    "* GooglePlus (numeric): Final value of the news items' popularity according to the social media source Google+\n",
    "* LinkedIn (numeric): Final value of the news items' popularity according to the social media source LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:21.068923Z",
     "start_time": "2018-04-13T06:13:21.062324Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_csv(file_name):\n",
    "    try:\n",
    "        data = sql_sc.read.csv(file_name, \n",
    "                       sep=',', \n",
    "                       header=True, \n",
    "                       mode='DROPMALFORMED')\n",
    "    except AnalysisException:\n",
    "        data = sql_sc.read.csv('hdfs:///bdm/hw2/{}'.format(file_name), \n",
    "                       sep=',', \n",
    "                       header=True, \n",
    "                       mode='DROPMALFORMED')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:25.058615Z",
     "start_time": "2018-04-13T06:13:21.073259Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "news = read_csv(files['news'])\n",
    "# news = (sql_sc.read.load(files['news'], \n",
    "#                          format=\"csv\", \n",
    "#                          schema=StructType([StructField(\"IDLink\", StringType(), False),\n",
    "#                                             StructField(\"Title\", StringType(), False),\n",
    "#                                             StructField(\"Headline\", StringType(), False),\n",
    "#                                             StructField(\"Source\", StringType(), False),\n",
    "#                                             StructField(\"Topic\", StringType(), False),\n",
    "#                                             StructField(\"PublishDate\", StringType(), False),\n",
    "#                                             StructField(\"SentimentTitle\", StringType(), False),\n",
    "#                                             StructField(\"SentimentHeadline\", StringType(), False),\n",
    "#                                             StructField(\"Facebook\", StringType(), False),\n",
    "#                                             StructField(\"GooglePlus\", StringType(), False),\n",
    "#                                             StructField(\"LinkedIn\", StringType(), False)]),\n",
    "#                          mode=\"DROPMALFORMED\", \n",
    "#                          header=\"true\")\n",
    "#         .drop('IDLink')\n",
    "#         .drop('Source')\n",
    "#         .drop('SentimentTitle')\n",
    "#         .drop('SentimentHeadline')\n",
    "#         .drop('Facebook')\n",
    "#         .drop('GooglePlus')\n",
    "#         .drop('LinkedIn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:25.063746Z",
     "start_time": "2018-04-13T06:13:25.061128Z"
    }
   },
   "outputs": [],
   "source": [
    "# news = news.sample(False, 0.01, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:26.534102Z",
     "start_time": "2018-04-13T06:13:25.066901Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88916"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:26.602387Z",
     "start_time": "2018-04-13T06:13:26.536763Z"
    }
   },
   "outputs": [],
   "source": [
    "news = news.withColumn('SentimentScore', (news.SentimentTitle+news.SentimentHeadline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:26.657489Z",
     "start_time": "2018-04-13T06:13:26.604846Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_data = (news.dropna()\n",
    "                .select('title', \n",
    "                        'headline', \n",
    "                        'topic', \n",
    "                        'publishDate',\n",
    "                        'SentimentScore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:26.665108Z",
     "start_time": "2018-04-13T06:13:26.660056Z"
    }
   },
   "outputs": [],
   "source": [
    "def wordTokenizer(data, columns):\n",
    "    for c in columns:\n",
    "        new_c = c + '_tokens'\n",
    "        reTokenizer = RegexTokenizer(inputCol=c, \n",
    "                                     outputCol=new_c, \n",
    "                                     pattern='\\\\W', \n",
    "                                     minTokenLength=2)\n",
    "        data = reTokenizer.transform(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:26.807754Z",
     "start_time": "2018-04-13T06:13:26.668350Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "col =  ['title', 'headline']\n",
    "news_data = wordTokenizer(news_data, col)\n",
    "news_data = news_data.select('title_tokens', \n",
    "                             'headline_tokens', \n",
    "                             'topic',  \n",
    "                             'publishDate',\n",
    "                             'SentimentScore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:26.846386Z",
     "start_time": "2018-04-13T06:13:26.810837Z"
    }
   },
   "outputs": [],
   "source": [
    "news_data = news_data.withColumn('publishDate', \n",
    "                                 udf(lambda tmp: tmp[:10] , StringType())\n",
    "                                 (news_data.publishDate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:27.945180Z",
     "start_time": "2018-04-13T06:13:26.848932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88622"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:28.445803Z",
     "start_time": "2018-04-13T06:13:27.948089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88916"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:28.451250Z",
     "start_time": "2018-04-13T06:13:28.448100Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# news_data = news_data.dropna()\n",
    "# news_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:29.121512Z",
     "start_time": "2018-04-13T06:13:28.453917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88622"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In news data, count the words in two fields: ‘Title’ and ‘Headline’ respectively, and list the most frequent words according to the term frequency in descending order, in total, per day, and per topic, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:29.129129Z",
     "start_time": "2018-04-13T06:13:29.123967Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def word_count_total(data, column, n=10):\n",
    "    return (news_data.select(column)\n",
    "            .rdd\n",
    "            .flatMap(lambda tokens: tokens[column])\n",
    "            .map(lambda word: (word, 1))\n",
    "            .reduceByKey(lambda a, b: a + b)\n",
    "            .sortBy(lambda w: w[1], ascending=False)\n",
    "            .take(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:29.134863Z",
     "start_time": "2018-04-13T06:13:29.131221Z"
    }
   },
   "outputs": [],
   "source": [
    "task1_file = open('result/task1/output.txt', 'w', encoding='utf-8', newline='\\n')\n",
    "task1_output = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:32.893050Z",
     "start_time": "2018-04-13T06:13:29.137334Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "task1_output.append('[title top-frequent words in total]')\n",
    "for r in word_count_total(news_data, 'title_tokens', n=100):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:36.540242Z",
     "start_time": "2018-04-13T06:13:32.895497Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "task1_output.append('\\n[headline top-frequent words in total]')\n",
    "for r in word_count_total(news_data, 'headline_tokens', n=100):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T03:53:33.945638Z",
     "start_time": "2018-04-12T03:53:29.739960Z"
    },
    "scrolled": true
   },
   "source": [
    " #### per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:36.547607Z",
     "start_time": "2018-04-13T06:13:36.542580Z"
    }
   },
   "outputs": [],
   "source": [
    "def sort(tokens):\n",
    "    take = 100 if len(tokens) >= 100 else len(tokens)\n",
    "    return sorted(tokens, key=lambda x: x[1], reverse=True)[:take]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:36.559231Z",
     "start_time": "2018-04-13T06:13:36.550116Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_count_per(data, column, per_col, take=-1):\n",
    "#     rdd = (news_data.select(column, per_col)\n",
    "#             .rdd\n",
    "#             .flatMap(lambda row: [((row[per_col], w), 1) for w in row[column]])\n",
    "#             .reduceByKey(lambda a, b: a + b)\n",
    "#             .map(lambda pair: (pair[0][0], (pair[0][1], pair[1])))\n",
    "#             .reduceByKey(lambda a, b: a if a[1] > b[1] else b)\n",
    "#             .sortBy(lambda w: w[1][1], ascending=False)\n",
    "#             )\n",
    "    rdd = (news_data.select(column, per_col)\n",
    "        .rdd\n",
    "        .flatMap(lambda row: [((row[per_col], w), 1) for w in row[column]])\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "        .map(lambda pair: (pair[0][0], (pair[0][1], pair[1])))\n",
    "        .groupByKey()\n",
    "        .map(lambda topic: (topic[0], sort(topic[1])))\n",
    "        .sortBy(lambda w: w[1][1], ascending=False)\n",
    "        )\n",
    "    if take == -1:\n",
    "        return rdd.collect()\n",
    "    else:\n",
    "        return rdd.take(take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:41.580206Z",
     "start_time": "2018-04-13T06:13:36.561856Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "task1_output.append('\\n[title top-frequent words per day]')\n",
    "for r in sorted(word_count_per(news_data, 'title_tokens', 'publishDate'), key=lambda x: x[0]):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:50.661100Z",
     "start_time": "2018-04-13T06:13:41.583436Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task1_output.append('\\n[headline top-frequent words per day]')\n",
    "for r in sorted(word_count_per(news_data, 'headline_tokens', 'publishDate'), key=lambda x: x[0]):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:53.054193Z",
     "start_time": "2018-04-13T06:13:50.663483Z"
    }
   },
   "outputs": [],
   "source": [
    "task1_output.append('\\n[title top-frequent words per topic]')\n",
    "for r in word_count_per(news_data, 'title_tokens', 'topic'):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:57.827483Z",
     "start_time": "2018-04-13T06:13:53.056404Z"
    }
   },
   "outputs": [],
   "source": [
    "task1_output.append('\\n[headline top-frequent words per topic]')\n",
    "for r in word_count_per(news_data, 'headline_tokens', 'topic'):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:57.861011Z",
     "start_time": "2018-04-13T06:13:57.829890Z"
    }
   },
   "outputs": [],
   "source": [
    "task1_file.writelines(['{}\\n'.format(r) for r in task1_output])\n",
    "task1_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In social feedback data, calculate the average popularity of each news by hour, and by day, respectively (for each platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:57.866895Z",
     "start_time": "2018-04-13T06:13:57.862972Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_social_data(data, files):\n",
    "    for f in files:\n",
    "        df = read_csv(f)\n",
    "        data = data.union(df) if data else df\n",
    "    for i in range(1, 144+1):\n",
    "        col_name = 'TS{}'.format(i)\n",
    "        data = data.withColumn(col_name, data[col_name].cast('int'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:13:57.872193Z",
     "start_time": "2018-04-13T06:13:57.868942Z"
    }
   },
   "outputs": [],
   "source": [
    "fb_social_data = google_social_data = linkedin_social_data = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:14:02.879938Z",
     "start_time": "2018-04-13T06:13:57.874211Z"
    }
   },
   "outputs": [],
   "source": [
    "fb_social_data = create_social_data(fb_social_data, files['fb'])\n",
    "google_social_data = create_social_data(google_social_data, files['google'])\n",
    "linkedin_social_data = create_social_data(linkedin_social_data, files['linkedin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:14:03.013550Z",
     "start_time": "2018-04-13T06:14:02.881768Z"
    }
   },
   "outputs": [],
   "source": [
    "fb_social_data = fb_social_data.dropna()\n",
    "google_social_data = google_social_data.dropna()\n",
    "linkedin_social_data = linkedin_social_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:14:03.018395Z",
     "start_time": "2018-04-13T06:14:03.015449Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_avg(seq):\n",
    "    sum_ = np.sum(seq)\n",
    "    return sum_/48, sum_/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:14:03.024118Z",
     "start_time": "2018-04-13T06:14:03.020676Z"
    }
   },
   "outputs": [],
   "source": [
    "def avg_popu(data, by=3):\n",
    "    return (data.rdd\n",
    "            .map(lambda r: (r['IDLink'],  get_avg(r[1:])))\n",
    "            .sortByKey()\n",
    "            .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:14:15.691110Z",
     "start_time": "2018-04-13T06:14:03.026252Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fb_avg_by_hour_and_day = avg_popu(fb_social_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:14:25.744044Z",
     "start_time": "2018-04-13T06:14:15.693514Z"
    }
   },
   "outputs": [],
   "source": [
    "google_avg_by_hour_and_day = avg_popu(google_social_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:14:34.835182Z",
     "start_time": "2018-04-13T06:14:25.746257Z"
    }
   },
   "outputs": [],
   "source": [
    "linkedin_avg_by_hour_and_day = avg_popu(linkedin_social_data, by=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:14:34.840362Z",
     "start_time": "2018-04-13T06:14:34.837458Z"
    }
   },
   "outputs": [],
   "source": [
    "avg_popularity = {'fb':fb_avg_by_hour_and_day,\n",
    "                 'google':google_avg_by_hour_and_day,\n",
    "                 'linkedin':linkedin_avg_by_hour_and_day}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:14:34.848304Z",
     "start_time": "2018-04-13T06:14:34.842970Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_csv(file_name, data):\n",
    "    with open(file_name, 'w', \n",
    "              encoding='utf-8', newline='\\n') as csvfile:\n",
    "        fieldnames = ['IDLink', 'avg_popularity']\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        writer.writerow(fieldnames)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:14:35.480663Z",
     "start_time": "2018-04-13T06:14:34.850536Z"
    }
   },
   "outputs": [],
   "source": [
    "for platform, data in avg_popularity.items():\n",
    "    rows_by_hour = []\n",
    "    rows_by_day = []\n",
    "    \n",
    "    for ID, (avg_by_hour, avg_by_day) in data:\n",
    "        rows_by_hour.append((ID, avg_by_hour))\n",
    "        rows_by_day.append( (ID, avg_by_day))\n",
    "        \n",
    "    save_csv('./result/task2/{}_avg_popularity_by_hour.csv'.format(platform), \n",
    "             rows_by_hour)\n",
    "    save_csv('./result/task2/{}_avg_popularity_by_day.csv'.format(platform), \n",
    "             rows_by_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  In news data, calculate the sum and average sentiment score of each topic, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:14:35.487329Z",
     "start_time": "2018-04-13T06:14:35.483351Z"
    }
   },
   "outputs": [],
   "source": [
    "task3_file = open('result/task3/output.txt', 'w', encoding='utf-8', newline='\\n')\n",
    "task3_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:14:36.151132Z",
     "start_time": "2018-04-13T06:14:35.490032Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sum_of_score = (news.select('SentimentScore', 'topic')\n",
    "                .rdd\n",
    "                .map(lambda r: (r['topic'], (r['SentimentScore'], 1)))\n",
    "                .reduceByKey(lambda a, b: (a[0]+b[0], a[1]+b[1]))\n",
    "                .map(lambda x: (x[0], x[1][0], (x[1][0]/x[1][1])))\n",
    "#                 .groupByKey()\n",
    "#                 .map(lambda topic: (topic[0], \n",
    "#                                     np.sum(list(topic[1])), \n",
    "#                                     np.mean(list(topic[1]))))\n",
    "                .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:14:36.157848Z",
     "start_time": "2018-04-13T06:14:36.153165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "economy   , -1608.328, -0.050006\n",
      "microsoft , -269.381, -0.012589\n",
      "palestine , -527.654, -0.063726\n",
      "obama     , -497.583, -0.018378\n"
     ]
    }
   ],
   "source": [
    "task3_output.append('[sum sentiment score of each topic]')\n",
    "for topic, sum_, avg in sum_of_score:\n",
    "    print('{:10s}, {:.3f}, {:.6f}'.format(topic, sum_, avg))\n",
    "    task3_output.append('{:10s}, {:.3f}'.format(topic, sum_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:14:36.164236Z",
     "start_time": "2018-04-13T06:14:36.160751Z"
    }
   },
   "outputs": [],
   "source": [
    "task3_output.append('[avg sentiment score of each topic]')\n",
    "for topic, sum_, avg in sum_of_score:\n",
    "    task3_output.append('{:>10s}, {:.6f}'.format(topic, avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:14:36.171424Z",
     "start_time": "2018-04-13T06:14:36.167544Z"
    }
   },
   "outputs": [],
   "source": [
    "task3_file.writelines(['{}\\n'.format(r) for r in task3_output])\n",
    "task3_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From subtask (1), for the top-100 frequent words per topic in titles and headlines, calculate their co-occurrence matrices (100x100), respectively. Each entry in the matrix will contain the co-occurrence frequency in all news titles and headlines, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:14:43.376995Z",
     "start_time": "2018-04-13T06:14:36.173363Z"
    }
   },
   "outputs": [],
   "source": [
    "fw_all = {'title_tokens':{topic:[w[0] for w in top] \n",
    "                          for topic, top in word_count_per(news_data, 'title_tokens', 'topic')}, \n",
    "         'headline_tokens':{topic:[w[0] for w in top] \n",
    "                          for topic, top in word_count_per(news_data, 'headline_tokens', 'topic')}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:14:43.384186Z",
     "start_time": "2018-04-13T06:14:43.379397Z"
    }
   },
   "outputs": [],
   "source": [
    "def counter(vocabulary, tokens):\n",
    "    return  [int(tokens.count(v) > 0) for v in vocabulary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:15:19.402824Z",
     "start_time": "2018-04-13T06:14:43.386865Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column name:title_tokens, topic:obama\n",
      "column name:title_tokens, topic:microsoft\n",
      "column name:title_tokens, topic:economy\n",
      "column name:title_tokens, topic:palestine\n",
      "column name:headline_tokens, topic:palestine\n",
      "column name:headline_tokens, topic:obama\n",
      "column name:headline_tokens, topic:microsoft\n",
      "column name:headline_tokens, topic:economy\n"
     ]
    }
   ],
   "source": [
    "for col_name, v in fw_all.items():\n",
    "    for topic, vocabulary in v.items():\n",
    "        print('column name:{}, topic:{}'.format(col_name, topic))\n",
    "        \n",
    "        X = np.array(news_data.select(col_name, 'topic')\n",
    "                     .rdd\n",
    "                     .filter(lambda r: r['topic'] == topic)\n",
    "                     .map(lambda r:counter(vocabulary, r[col_name]))\n",
    "                     .collect(), dtype='int64')\n",
    "        co_occ = X.T.dot(X)\n",
    "        # np.fill_diagonal(co_occ, 0)\n",
    "        df = pd.DataFrame(data=co_occ, columns=vocabulary, index=vocabulary)\n",
    "        # display(df)\n",
    "        df.to_csv('result/task4/{}_{}_matrix.csv'.format(col_name, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T06:15:19.409416Z",
     "start_time": "2018-04-13T06:15:19.405088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost 2.02 minutes\n"
     ]
    }
   ],
   "source": [
    "print('cost {:.2f} minutes'.format((time.time()-t0)/60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "171px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
