{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 103590450 四資四 馬茂源"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](1.png)\n",
    "![](2.png)\n",
    "![](3.png)\n",
    "![](4.png)\n",
    "![](5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:24:13.693656Z",
     "start_time": "2018-04-12T18:24:13.404890Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.sql.functions import udf, mean\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, math, time\n",
    "import itertools\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:24:13.698168Z",
     "start_time": "2018-04-12T18:24:13.694659Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('result'):\n",
    "    os.mkdir('result')\n",
    "for i in range(1, 5):\n",
    "    dir_ = 'result/task{}'.format(i)\n",
    "    if not os.path.exists(dir_):\n",
    "        os.mkdir(dir_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:24:13.706190Z",
     "start_time": "2018-04-12T18:24:13.699171Z"
    }
   },
   "outputs": [],
   "source": [
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:24:13.712206Z",
     "start_time": "2018-04-12T18:24:13.707192Z"
    }
   },
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "        #.setMaster('spark://10.100.5.182:7077')\n",
    "        #.setMaster(\"local\")\n",
    "        .setAppName(\"hw2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:24:15.620277Z",
     "start_time": "2018-04-12T18:24:13.713209Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sc = SparkContext(conf=conf)\n",
    "    sql_sc = SQLContext(sc)\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:24:15.626794Z",
     "start_time": "2018-04-12T18:24:15.621280Z"
    }
   },
   "outputs": [],
   "source": [
    "files = {'fb':['Facebook_Economy.csv', \n",
    "               'Facebook_Obama.csv', \n",
    "               'Facebook_Palestine.csv', \n",
    "               'Facebook_Microsoft.csv'],\n",
    "        'google':['GooglePlus_Obama.csv', \n",
    "                  'GooglePlus_Palestine.csv', \n",
    "                  'GooglePlus_Economy.csv', \n",
    "                  'GooglePlus_Microsoft.csv'],\n",
    "        'linkedin':['LinkedIn_Microsoft.csv', \n",
    "                    'LinkedIn_Palestine.csv',\n",
    "                    'LinkedIn_Obama.csv', \n",
    "                    'LinkedIn_Economy.csv'],\n",
    "        'news':'News_Final.csv'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* IDLink (numeric): Unique identifier of news items\n",
    "* Title (string): Title of the news item according to the official media sources\n",
    "* Headline (string): Headline of the news item according to the official media sources\n",
    "* Source (string): Original news outlet that published the news item\n",
    "* Topic (string): Query topic used to obtain the items in the official media sources\n",
    "* PublishDate (timestamp): Date and time of the news items' publication\n",
    "* SentimentTitle (numeric): Sentiment score of the text in the news items' title\n",
    "* SentimentHeadline (numeric): Sentiment score of the text in the news items' headline\n",
    "* Facebook (numeric): Final value of the news items' popularity according to the social media source Facebook\n",
    "* GooglePlus (numeric): Final value of the news items' popularity according to the social media source Google+\n",
    "* LinkedIn (numeric): Final value of the news items' popularity according to the social media source LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:24:15.634314Z",
     "start_time": "2018-04-12T18:24:15.627798Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_csv(file_name):\n",
    "    try:\n",
    "        data = sql_sc.read.csv(file_name, \n",
    "                       sep=',', \n",
    "                       header=True, \n",
    "                       mode='DROPMALFORMED')\n",
    "    except AnalysisException:\n",
    "        data = sql_sc.read.csv('hdfs:///bdm/hw2/{}'.format(file_name), \n",
    "                       sep=',', \n",
    "                       header=True, \n",
    "                       mode='DROPMALFORMED')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:24:18.349531Z",
     "start_time": "2018-04-12T18:24:15.635318Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "news = read_csv(files['news'])\n",
    "# news = (sql_sc.read.load(files['news'], \n",
    "#                          format=\"csv\", \n",
    "#                          schema=StructType([StructField(\"IDLink\", StringType(), False),\n",
    "#                                             StructField(\"Title\", StringType(), False),\n",
    "#                                             StructField(\"Headline\", StringType(), False),\n",
    "#                                             StructField(\"Source\", StringType(), False),\n",
    "#                                             StructField(\"Topic\", StringType(), False),\n",
    "#                                             StructField(\"PublishDate\", StringType(), False),\n",
    "#                                             StructField(\"SentimentTitle\", StringType(), False),\n",
    "#                                             StructField(\"SentimentHeadline\", StringType(), False),\n",
    "#                                             StructField(\"Facebook\", StringType(), False),\n",
    "#                                             StructField(\"GooglePlus\", StringType(), False),\n",
    "#                                             StructField(\"LinkedIn\", StringType(), False)]),\n",
    "#                          mode=\"DROPMALFORMED\", \n",
    "#                          header=\"true\")\n",
    "#         .drop('IDLink')\n",
    "#         .drop('Source')\n",
    "#         .drop('SentimentTitle')\n",
    "#         .drop('SentimentHeadline')\n",
    "#         .drop('Facebook')\n",
    "#         .drop('GooglePlus')\n",
    "#         .drop('LinkedIn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:24:18.352539Z",
     "start_time": "2018-04-12T18:24:18.350534Z"
    }
   },
   "outputs": [],
   "source": [
    "# news = news.sample(False, 0.01, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:24:18.397659Z",
     "start_time": "2018-04-12T18:24:18.354044Z"
    }
   },
   "outputs": [],
   "source": [
    "news = news.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:24:18.414705Z",
     "start_time": "2018-04-12T18:24:18.398662Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_data = news.select('title', \n",
    "                        'headline', \n",
    "                        'topic', \n",
    "                        'publishDate',\n",
    "                        'SentimentTitle', \n",
    "                        'SentimentHeadline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:24:18.419718Z",
     "start_time": "2018-04-12T18:24:18.415707Z"
    }
   },
   "outputs": [],
   "source": [
    "def wordTokenizer(data, columns):\n",
    "    for c in columns:\n",
    "        new_c = c + '_tokens'\n",
    "        reTokenizer = RegexTokenizer(inputCol=c, \n",
    "                                     outputCol=new_c, \n",
    "                                     pattern='\\\\W', \n",
    "                                     minTokenLength=2)\n",
    "        data = reTokenizer.transform(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:24:18.477872Z",
     "start_time": "2018-04-12T18:24:18.420721Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "col =  ['title', 'headline']\n",
    "news_data = wordTokenizer(news_data, col)\n",
    "news_data = news_data.select('title_tokens', \n",
    "                             'headline_tokens', \n",
    "                             'topic',  \n",
    "                             'publishDate',\n",
    "                             'SentimentTitle', \n",
    "                             'SentimentHeadline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:24:18.495920Z",
     "start_time": "2018-04-12T18:24:18.478875Z"
    }
   },
   "outputs": [],
   "source": [
    "news_data = news_data.withColumn('publishDate', \n",
    "                                 udf(lambda tmp: tmp[:10] , StringType())\n",
    "                                 (news_data.publishDate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:24:18.542544Z",
     "start_time": "2018-04-12T18:24:18.496923Z"
    }
   },
   "outputs": [],
   "source": [
    "news_data = news_data.withColumn('SentimentScore', (news_data.SentimentTitle+news_data.SentimentHeadline)/2)\n",
    "news_data = news_data.select('title_tokens', \n",
    "                             'headline_tokens', \n",
    "                             'topic',  \n",
    "                             'publishDate',\n",
    "                             'SentimentScore')                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:24:19.839492Z",
     "start_time": "2018-04-12T18:24:18.543547Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+-----------+--------------------+\n",
      "|        title_tokens|     headline_tokens|    topic|publishDate|      SentimentScore|\n",
      "+--------------------+--------------------+---------+-----------+--------------------+\n",
      "|[obama, lays, wre...|[obama, lays, wre...|    obama| 2002-04-02| -0.0266500895444513|\n",
      "|[look, at, the, h...|[tim, haywood, in...|  economy| 2008-09-20|  0.0259737613952635|\n",
      "|[nouriel, roubini...|[nouriel, roubini...|  economy| 2012-01-28|  -0.142727891770822|\n",
      "|[finland, gdp, ex...|[finland, economy...|  economy| 2015-03-01| 0.01303215087856715|\n",
      "|[tourism, govt, s...|[tourism, and, pu...|  economy| 2015-03-01|  0.0705422282441575|\n",
      "|[intellitec, solu...|[over, 100, atten...|microsoft| 2015-03-01|-0.01930252131229...|\n",
      "|[obama, stars, pa...|[first, lady, mic...|    obama| 2015-03-01| 0.09316804504927514|\n",
      "|[fire, claims, mo...|[hancock, county,...|palestine| 2015-03-01|-0.11205498154672319|\n",
      "|[microsoft, new, ...|[new, delhi, feb,...|microsoft| 2015-03-01|-0.07062563266677825|\n",
      "|[microsoft, proje...|[microsoft, may, ...|microsoft| 2015-03-01| 0.00127497041897463|\n",
      "|[microsoft, sneak...|[the, platform, b...|microsoft| 2015-03-01| 0.02633513150296405|\n",
      "|[greek, economy, ...|[greece, economy,...|  economy| 2015-03-01|  -0.187629664616396|\n",
      "|[big, data, and, ...|[big, data, analy...|  economy| 2015-03-01| 0.05109553912555111|\n",
      "|[hololens, dev, e...|[microsoft, ar, h...|microsoft| 2015-03-01| 0.03971684226033435|\n",
      "|[microsoft, word,...|[what, is, welcom...|microsoft| 2015-03-01|-0.02807912926000375|\n",
      "|[microsoft, band,...|[the, microsoft, ...|microsoft| 2015-03-01|-0.03543416934461505|\n",
      "|[microsoft, prepa...|[it, seems, that,...|microsoft| 2015-03-01|        6.8359375E-4|\n",
      "|[greek, economy, ...|[greece, economy,...|  economy| 2015-03-01| -0.0925881457513656|\n",
      "|[sweden, economy,...|[sweden, economy,...|  economy| 2015-03-01|  -0.007086833868923|\n",
      "|[the, microsoft, ...|[the, microsoft, ...|microsoft| 2015-03-01|0.021093190275797053|\n",
      "+--------------------+--------------------+---------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_data = news_data.dropna()\n",
    "news_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In news data, count the words in two fields: ‘Title’ and ‘Headline’ respectively, and list the most frequent words according to the term frequency in descending order, in total, per day, and per topic, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:24:19.850020Z",
     "start_time": "2018-04-12T18:24:19.841498Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def word_count_total(data, column, n=10):\n",
    "    return (news_data.select(column)\n",
    "            .rdd\n",
    "            .flatMap(lambda tokens: tokens[column])\n",
    "            .map(lambda word: (word, 1))\n",
    "            .reduceByKey(lambda a, b: a + b)\n",
    "            .sortBy(lambda w: w[1], ascending=False)\n",
    "            .take(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:24:19.860548Z",
     "start_time": "2018-04-12T18:24:19.851523Z"
    }
   },
   "outputs": [],
   "source": [
    "task1_file = open('result/task1/output.txt', 'w', encoding='utf-8', newline='\\n')\n",
    "task1_output = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:24:35.405365Z",
     "start_time": "2018-04-12T18:24:19.862052Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "task1_output.append('[title top-frequent words in total]')\n",
    "for r in word_count_total(news_data, 'title_tokens', n=10):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:24:50.891526Z",
     "start_time": "2018-04-12T18:24:35.407873Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "task1_output.append('[headline top-frequent words in total]')\n",
    "for r in word_count_total(news_data, 'headline_tokens', n=10):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T03:53:33.945638Z",
     "start_time": "2018-04-12T03:53:29.739960Z"
    },
    "scrolled": true
   },
   "source": [
    " #### per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:24:50.913584Z",
     "start_time": "2018-04-12T18:24:50.894033Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_count_per(data, column, per_col, take=-1):\n",
    "    rdd = (news_data.select(column, per_col)\n",
    "            .rdd\n",
    "            .flatMap(lambda row: [((row[per_col], w), 1) for w in row[column]])\n",
    "            .reduceByKey(lambda a, b: a + b)\n",
    "            .map(lambda pair: (pair[0][0], (pair[0][1], pair[1])))\n",
    "            .reduceByKey(lambda a, b: a if a[1] > b[1] else b)\n",
    "            .sortBy(lambda w: w[1][1], ascending=False)\n",
    "            )\n",
    "    if take == -1:\n",
    "        return rdd.collect()\n",
    "    else:\n",
    "        return rdd.take(take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:25:12.570647Z",
     "start_time": "2018-04-12T18:24:50.915590Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task1_output.append('[title top-frequent words per day]')\n",
    "for r in word_count_per(news_data, 'title_tokens', 'publishDate', take=60):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:25:34.894265Z",
     "start_time": "2018-04-12T18:25:12.572653Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task1_output.append('[headline top-frequent words per day]')\n",
    "for r in word_count_per(news_data, 'headline_tokens', 'publishDate', take=60):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:25:54.135908Z",
     "start_time": "2018-04-12T18:25:34.896773Z"
    }
   },
   "outputs": [],
   "source": [
    "task1_output.append('[title top-frequent words per topic]')\n",
    "for r in word_count_per(news_data, 'title_tokens', 'topic'):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:26:13.614681Z",
     "start_time": "2018-04-12T18:25:54.138415Z"
    }
   },
   "outputs": [],
   "source": [
    "task1_output.append('[headline top-frequent words per topic]')\n",
    "for r in word_count_per(news_data, 'headline_tokens', 'topic'):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:26:13.630223Z",
     "start_time": "2018-04-12T18:26:13.617189Z"
    }
   },
   "outputs": [],
   "source": [
    "task1_file.writelines(['{}\\n'.format(r) for r in task1_output])\n",
    "task1_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In social feedback data, calculate the average popularity of each news by hour, and by day, respectively (for each platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:26:13.638245Z",
     "start_time": "2018-04-12T18:26:13.633734Z"
    }
   },
   "outputs": [],
   "source": [
    "fb_social_data = google_social_data = linkedin_social_data = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:26:13.654287Z",
     "start_time": "2018-04-12T18:26:13.641253Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_social_data(data, files):\n",
    "    for f in files:\n",
    "        df = read_csv(f)\n",
    "        data = data.union(df) if data else df\n",
    "    for i in range(1, 144+1):\n",
    "        col_name = 'TS{}'.format(i)\n",
    "        data = data.withColumn(col_name, data[col_name].cast('int'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:26:17.735639Z",
     "start_time": "2018-04-12T18:26:13.655289Z"
    }
   },
   "outputs": [],
   "source": [
    "fb_social_data = create_social_data(fb_social_data, files['fb'])\n",
    "google_social_data = create_social_data(google_social_data, files['google'])\n",
    "linkedin_social_data = create_social_data(linkedin_social_data, files['linkedin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:26:17.818358Z",
     "start_time": "2018-04-12T18:26:17.736641Z"
    }
   },
   "outputs": [],
   "source": [
    "fb_social_data = fb_social_data.dropna()\n",
    "google_social_data = google_social_data.dropna()\n",
    "linkedin_social_data = linkedin_social_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:26:17.821367Z",
     "start_time": "2018-04-12T18:26:17.819361Z"
    }
   },
   "outputs": [],
   "source": [
    "hour = 3\n",
    "day = 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:26:17.827383Z",
     "start_time": "2018-04-12T18:26:17.822370Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_avg(seq):\n",
    "    sum_ = np.sum(seq)\n",
    "    return sum_/48, sum_/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:26:17.834401Z",
     "start_time": "2018-04-12T18:26:17.828385Z"
    }
   },
   "outputs": [],
   "source": [
    "def avg_popu(data, by=3):\n",
    "#     return (data\n",
    "#             .rdd\n",
    "#             .map(lambda r: (r['IDLink'], \n",
    "#                                  [np.mean(chunk) for chunk in zip(*[iter(r[1:])]*by)]))\n",
    "#             .collect())\n",
    "    return (data\n",
    "           .rdd\n",
    "           .map(lambda r: (r['IDLink'],  get_avg(r[1:])))\n",
    "           .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:26:23.106417Z",
     "start_time": "2018-04-12T18:26:17.835404Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fb_avg_by_hour_and_day = avg_popu(fb_social_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:26:27.726199Z",
     "start_time": "2018-04-12T18:26:23.107420Z"
    }
   },
   "outputs": [],
   "source": [
    "google_avg_by_hour_and_day = avg_popu(google_social_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:26:32.366535Z",
     "start_time": "2018-04-12T18:26:27.727199Z"
    }
   },
   "outputs": [],
   "source": [
    "linkedin_avg_by_hour_and_day = avg_popu(linkedin_social_data, by=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:26:32.369543Z",
     "start_time": "2018-04-12T18:26:32.367538Z"
    }
   },
   "outputs": [],
   "source": [
    "avg_popularity = {'fb':fb_avg_by_hour_and_day,\n",
    "                 'google':google_avg_by_hour_and_day,\n",
    "                 'linkedin':linkedin_avg_by_hour_and_day}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:26:32.380572Z",
     "start_time": "2018-04-12T18:26:32.370546Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_csv(file_name, data):\n",
    "    with open(file_name, 'w', \n",
    "              encoding='utf-8', newline='\\n') as csvfile:\n",
    "        fieldnames = ['IDLink', 'avg_popularity']\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        writer.writerow(fieldnames)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:26:32.846310Z",
     "start_time": "2018-04-12T18:26:32.381575Z"
    }
   },
   "outputs": [],
   "source": [
    "for platform, data in avg_popularity.items():\n",
    "    rows_by_hour = []\n",
    "    rows_by_day = []\n",
    "    \n",
    "    for ID, (avg_by_hour, avg_by_day) in data:\n",
    "        rows_by_hour.append((ID, avg_by_hour))\n",
    "        rows_by_day.append( (ID, avg_by_day))\n",
    "        \n",
    "    save_csv('./result/task2/{}_avg_popularity_by_hour.csv'.format(platform), \n",
    "             rows_by_hour)\n",
    "    save_csv('./result/task2/{}_avg_popularity_by_day.csv'.format(platform), \n",
    "             rows_by_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  In news data, calculate the sum and average sentiment score of each topic, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:26:32.850321Z",
     "start_time": "2018-04-12T18:26:32.847313Z"
    }
   },
   "outputs": [],
   "source": [
    "task3_file = open('result/task3/output.txt', 'w', encoding='utf-8', newline='\\n')\n",
    "task3_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:26:40.922275Z",
     "start_time": "2018-04-12T18:26:32.851324Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sum_of_score = (news_data.select('SentimentScore', 'topic')\n",
    "                .rdd\n",
    "                .map(lambda r: (r['topic'], r['SentimentScore']))\n",
    "                .groupByKey()\n",
    "                .map(lambda topic: (topic[0], list(topic[1])))\n",
    "                .map(lambda topic: (topic[0], np.sum(topic[1]), np.mean(topic[1])))\n",
    "                .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:26:40.926787Z",
     "start_time": "2018-04-12T18:26:40.923279Z"
    }
   },
   "outputs": [],
   "source": [
    "task3_output.append('[sum sentiment score of each topic]')\n",
    "for topic, sum_, avg in sum_of_score:\n",
    "    #print('{:>10s}, {:.3f}'.format(*topic_row))\n",
    "    task3_output.append('{:>10s}, {:.3f}'.format(topic, sum_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:26:40.933806Z",
     "start_time": "2018-04-12T18:26:40.927790Z"
    }
   },
   "outputs": [],
   "source": [
    "task3_output.append('[avg sentiment score of each topic]')\n",
    "for topic, sum_, avg in sum_of_score:\n",
    "    task3_output.append('{:>10s}, {:.6f}'.format(topic, avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:26:40.941827Z",
     "start_time": "2018-04-12T18:26:40.934809Z"
    }
   },
   "outputs": [],
   "source": [
    "task3_file.writelines(['{}\\n'.format(r) for r in task3_output])\n",
    "task3_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From subtask (1), for the top-100 frequent words per topic in titles and headlines, calculate their co-occurrence matrices (100x100), respectively. Each entry in the matrix will contain the co-occurrence frequency in all news titles and headlines, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:26:40.950350Z",
     "start_time": "2018-04-12T18:26:40.943332Z"
    }
   },
   "outputs": [],
   "source": [
    "def top_100(data, column, per_col):\n",
    "    return (news_data.select(column, per_col)\n",
    "            .rdd\n",
    "            .flatMap(lambda row: [((row[per_col], w), 1) for w in row[column]])\n",
    "            .reduceByKey(lambda a, b: a + b)\n",
    "            .map(lambda pair: (pair[0][0], (pair[0][1], pair[1])))\n",
    "            .groupByKey()\n",
    "            .map(lambda topic: (topic[0], sorted(topic[1], \n",
    "                                                 key=lambda x: x[1], \n",
    "                                                 reverse=True)[:100]))\n",
    "            .map(lambda topic: (topic[0], [w[0] for w in topic[1]]))\n",
    "            .collect()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:27:02.207851Z",
     "start_time": "2018-04-12T18:26:40.951353Z"
    }
   },
   "outputs": [],
   "source": [
    "fw_all = {'title_tokens':dict(top_100(news_data, 'title_tokens', 'topic')), \n",
    "         'headline_tokens':dict(top_100(news_data, 'headline_tokens', 'topic'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:27:02.214870Z",
     "start_time": "2018-04-12T18:27:02.210358Z"
    }
   },
   "outputs": [],
   "source": [
    "def counter(vocabulary, tokens):\n",
    "    return  [int(tokens.count(v) > 0) for v in vocabulary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:27:57.841220Z",
     "start_time": "2018-04-12T18:27:02.217878Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col_name, v in fw_all.items():\n",
    "    for topic, vocabulary in v.items():\n",
    "        #print('column name:{}, topic:{}'.format(col_name, topic))\n",
    "        \n",
    "        X = np.array(news_data.select(col_name, 'topic')\n",
    "                     .rdd\n",
    "                     .filter(lambda r: r['topic'] == topic)\n",
    "                     .map(lambda r:counter(vocabulary, r[col_name]))\n",
    "                     .collect(), dtype='int64')\n",
    "        co_occ = X.T.dot(X)\n",
    "        np.fill_diagonal(co_occ, 0)\n",
    "        (pd.DataFrame(data=co_occ, columns=vocabulary, index=vocabulary)\n",
    "         .to_csv('result/task4/{}_{}_matrix.csv'.format(col_name, topic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:27:57.845732Z",
     "start_time": "2018-04-12T18:27:57.842724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost 3.74 minutes\n"
     ]
    }
   ],
   "source": [
    "print('cost {:.2f} minutes'.format((time.time()-t0)/60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "171px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
