{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 103590450 四資四 馬茂源"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](1.png)\n",
    "![](2.png)\n",
    "![](3.png)\n",
    "![](4.png)\n",
    "![](5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:04:48.590936Z",
     "start_time": "2018-04-13T03:04:47.823692Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.sql.functions import udf, mean\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, math, time\n",
    "import itertools\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:04:48.610638Z",
     "start_time": "2018-04-13T03:04:48.594920Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('result'):\n",
    "    os.mkdir('result')\n",
    "for i in range(1, 5):\n",
    "    dir_ = 'result/task{}'.format(i)\n",
    "    if not os.path.exists(dir_):\n",
    "        os.mkdir(dir_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:04:48.620214Z",
     "start_time": "2018-04-13T03:04:48.614524Z"
    }
   },
   "outputs": [],
   "source": [
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:04:48.626562Z",
     "start_time": "2018-04-13T03:04:48.623075Z"
    }
   },
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "        #.setMaster('spark://10.100.5.182:7077')\n",
    "        #.setMaster(\"local\")\n",
    "        .setAppName(\"hw2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:04:51.435923Z",
     "start_time": "2018-04-13T03:04:48.629276Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sc = SparkContext(conf=conf)\n",
    "    sql_sc = SQLContext(sc)\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:04:51.444782Z",
     "start_time": "2018-04-13T03:04:51.438423Z"
    }
   },
   "outputs": [],
   "source": [
    "files = {'fb':['Facebook_Economy.csv', \n",
    "               'Facebook_Obama.csv', \n",
    "               'Facebook_Palestine.csv', \n",
    "               'Facebook_Microsoft.csv'],\n",
    "        'google':['GooglePlus_Obama.csv', \n",
    "                  'GooglePlus_Palestine.csv', \n",
    "                  'GooglePlus_Economy.csv', \n",
    "                  'GooglePlus_Microsoft.csv'],\n",
    "        'linkedin':['LinkedIn_Microsoft.csv', \n",
    "                    'LinkedIn_Palestine.csv',\n",
    "                    'LinkedIn_Obama.csv', \n",
    "                    'LinkedIn_Economy.csv'],\n",
    "        'news':'News_Final.csv'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* IDLink (numeric): Unique identifier of news items\n",
    "* Title (string): Title of the news item according to the official media sources\n",
    "* Headline (string): Headline of the news item according to the official media sources\n",
    "* Source (string): Original news outlet that published the news item\n",
    "* Topic (string): Query topic used to obtain the items in the official media sources\n",
    "* PublishDate (timestamp): Date and time of the news items' publication\n",
    "* SentimentTitle (numeric): Sentiment score of the text in the news items' title\n",
    "* SentimentHeadline (numeric): Sentiment score of the text in the news items' headline\n",
    "* Facebook (numeric): Final value of the news items' popularity according to the social media source Facebook\n",
    "* GooglePlus (numeric): Final value of the news items' popularity according to the social media source Google+\n",
    "* LinkedIn (numeric): Final value of the news items' popularity according to the social media source LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:04:51.451479Z",
     "start_time": "2018-04-13T03:04:51.446961Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_csv(file_name):\n",
    "    try:\n",
    "        data = sql_sc.read.csv(file_name, \n",
    "                       sep=',', \n",
    "                       header=True, \n",
    "                       mode='DROPMALFORMED')\n",
    "    except AnalysisException:\n",
    "        data = sql_sc.read.csv('hdfs:///bdm/hw2/{}'.format(file_name), \n",
    "                       sep=',', \n",
    "                       header=True, \n",
    "                       mode='DROPMALFORMED')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:04:55.610143Z",
     "start_time": "2018-04-13T03:04:51.454146Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "news = read_csv(files['news'])\n",
    "# news = (sql_sc.read.load(files['news'], \n",
    "#                          format=\"csv\", \n",
    "#                          schema=StructType([StructField(\"IDLink\", StringType(), False),\n",
    "#                                             StructField(\"Title\", StringType(), False),\n",
    "#                                             StructField(\"Headline\", StringType(), False),\n",
    "#                                             StructField(\"Source\", StringType(), False),\n",
    "#                                             StructField(\"Topic\", StringType(), False),\n",
    "#                                             StructField(\"PublishDate\", StringType(), False),\n",
    "#                                             StructField(\"SentimentTitle\", StringType(), False),\n",
    "#                                             StructField(\"SentimentHeadline\", StringType(), False),\n",
    "#                                             StructField(\"Facebook\", StringType(), False),\n",
    "#                                             StructField(\"GooglePlus\", StringType(), False),\n",
    "#                                             StructField(\"LinkedIn\", StringType(), False)]),\n",
    "#                          mode=\"DROPMALFORMED\", \n",
    "#                          header=\"true\")\n",
    "#         .drop('IDLink')\n",
    "#         .drop('Source')\n",
    "#         .drop('SentimentTitle')\n",
    "#         .drop('SentimentHeadline')\n",
    "#         .drop('Facebook')\n",
    "#         .drop('GooglePlus')\n",
    "#         .drop('LinkedIn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:04:55.617736Z",
     "start_time": "2018-04-13T03:04:55.612498Z"
    }
   },
   "outputs": [],
   "source": [
    "# news = news.sample(False, 0.01, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:04:55.689839Z",
     "start_time": "2018-04-13T03:04:55.620038Z"
    }
   },
   "outputs": [],
   "source": [
    "news = news.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:04:55.723238Z",
     "start_time": "2018-04-13T03:04:55.692372Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_data = news.select('title', \n",
    "                        'headline', \n",
    "                        'topic', \n",
    "                        'publishDate',\n",
    "                        'SentimentTitle', \n",
    "                        'SentimentHeadline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:04:55.728629Z",
     "start_time": "2018-04-13T03:04:55.725103Z"
    }
   },
   "outputs": [],
   "source": [
    "def wordTokenizer(data, columns):\n",
    "    for c in columns:\n",
    "        new_c = c + '_tokens'\n",
    "        reTokenizer = RegexTokenizer(inputCol=c, \n",
    "                                     outputCol=new_c, \n",
    "                                     pattern='\\\\W', \n",
    "                                     minTokenLength=2)\n",
    "        data = reTokenizer.transform(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:04:55.827926Z",
     "start_time": "2018-04-13T03:04:55.731130Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "col =  ['title', 'headline']\n",
    "news_data = wordTokenizer(news_data, col)\n",
    "news_data = news_data.select('title_tokens', \n",
    "                             'headline_tokens', \n",
    "                             'topic',  \n",
    "                             'publishDate',\n",
    "                             'SentimentTitle', \n",
    "                             'SentimentHeadline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:04:55.855344Z",
     "start_time": "2018-04-13T03:04:55.829989Z"
    }
   },
   "outputs": [],
   "source": [
    "news_data = news_data.withColumn('publishDate', \n",
    "                                 udf(lambda tmp: tmp[:10] , StringType())\n",
    "                                 (news_data.publishDate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:04:55.913809Z",
     "start_time": "2018-04-13T03:04:55.857243Z"
    }
   },
   "outputs": [],
   "source": [
    "news_data = news_data.withColumn('SentimentScore', (news_data.SentimentTitle+news_data.SentimentHeadline))\n",
    "news_data = news_data.select('title_tokens', \n",
    "                             'headline_tokens', \n",
    "                             'topic',  \n",
    "                             'publishDate',\n",
    "                             'SentimentScore')                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:04:57.429722Z",
     "start_time": "2018-04-13T03:04:55.916096Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+-----------+--------------------+\n",
      "|        title_tokens|     headline_tokens|    topic|publishDate|      SentimentScore|\n",
      "+--------------------+--------------------+---------+-----------+--------------------+\n",
      "|[obama, lays, wre...|[obama, lays, wre...|    obama| 2002-04-02| -0.0533001790889026|\n",
      "|[look, at, the, h...|[tim, haywood, in...|  economy| 2008-09-20|   0.051947522790527|\n",
      "|[nouriel, roubini...|[nouriel, roubini...|  economy| 2012-01-28|  -0.285455783541644|\n",
      "|[finland, gdp, ex...|[finland, economy...|  economy| 2015-03-01|  0.0260643017571343|\n",
      "|[tourism, govt, s...|[tourism, and, pu...|  economy| 2015-03-01|   0.141084456488315|\n",
      "|[intellitec, solu...|[over, 100, atten...|microsoft| 2015-03-01|-0.03860504262458479|\n",
      "|[obama, stars, pa...|[first, lady, mic...|    obama| 2015-03-01|  0.1863360900985503|\n",
      "|[fire, claims, mo...|[hancock, county,...|palestine| 2015-03-01|-0.22410996309344638|\n",
      "|[microsoft, new, ...|[new, delhi, feb,...|microsoft| 2015-03-01| -0.1412512653335565|\n",
      "|[microsoft, proje...|[microsoft, may, ...|microsoft| 2015-03-01| 0.00254994083794926|\n",
      "|[microsoft, sneak...|[the, platform, b...|microsoft| 2015-03-01|  0.0526702630059281|\n",
      "|[greek, economy, ...|[greece, economy,...|  economy| 2015-03-01|  -0.375259329232792|\n",
      "|[big, data, and, ...|[big, data, analy...|  economy| 2015-03-01| 0.10219107825110221|\n",
      "|[hololens, dev, e...|[microsoft, ar, h...|microsoft| 2015-03-01|  0.0794336845206687|\n",
      "|[microsoft, word,...|[what, is, welcom...|microsoft| 2015-03-01| -0.0561582585200075|\n",
      "|[microsoft, band,...|[the, microsoft, ...|microsoft| 2015-03-01| -0.0708683386892301|\n",
      "|[microsoft, prepa...|[it, seems, that,...|microsoft| 2015-03-01|        0.0013671875|\n",
      "|[greek, economy, ...|[greece, economy,...|  economy| 2015-03-01| -0.1851762915027312|\n",
      "|[sweden, economy,...|[sweden, economy,...|  economy| 2015-03-01|  -0.014173667737846|\n",
      "|[the, microsoft, ...|[the, microsoft, ...|microsoft| 2015-03-01|0.042186380551594106|\n",
      "+--------------------+--------------------+---------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_data = news_data.dropna()\n",
    "news_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In news data, count the words in two fields: ‘Title’ and ‘Headline’ respectively, and list the most frequent words according to the term frequency in descending order, in total, per day, and per topic, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:04:57.442170Z",
     "start_time": "2018-04-13T03:04:57.432233Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def word_count_total(data, column, n=10):\n",
    "    return (news_data.select(column)\n",
    "            .rdd\n",
    "            .flatMap(lambda tokens: tokens[column])\n",
    "            .map(lambda word: (word, 1))\n",
    "            .reduceByKey(lambda a, b: a + b)\n",
    "            .sortBy(lambda w: w[1], ascending=False)\n",
    "            .take(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:04:57.447541Z",
     "start_time": "2018-04-13T03:04:57.444633Z"
    }
   },
   "outputs": [],
   "source": [
    "task1_file = open('result/task1/output.txt', 'w', encoding='utf-8', newline='\\n')\n",
    "task1_output = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:01.233910Z",
     "start_time": "2018-04-13T03:04:57.449662Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "task1_output.append('[title top-frequent words in total]')\n",
    "for r in word_count_total(news_data, 'title_tokens', n=100):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:05.039462Z",
     "start_time": "2018-04-13T03:05:01.236319Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "task1_output.append('\\n[headline top-frequent words in total]')\n",
    "for r in word_count_total(news_data, 'headline_tokens', n=100):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T03:53:33.945638Z",
     "start_time": "2018-04-12T03:53:29.739960Z"
    },
    "scrolled": true
   },
   "source": [
    " #### per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:05.048187Z",
     "start_time": "2018-04-13T03:05:05.042022Z"
    }
   },
   "outputs": [],
   "source": [
    "def sort(tokens):\n",
    "    take = 100 if len(tokens) >= 100 else len(tokens)\n",
    "    return sorted(tokens, key=lambda x: x[1], reverse=True)[:take]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:05.061058Z",
     "start_time": "2018-04-13T03:05:05.050809Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_count_per(data, column, per_col, take=-1):\n",
    "#     rdd = (news_data.select(column, per_col)\n",
    "#             .rdd\n",
    "#             .flatMap(lambda row: [((row[per_col], w), 1) for w in row[column]])\n",
    "#             .reduceByKey(lambda a, b: a + b)\n",
    "#             .map(lambda pair: (pair[0][0], (pair[0][1], pair[1])))\n",
    "#             .reduceByKey(lambda a, b: a if a[1] > b[1] else b)\n",
    "#             .sortBy(lambda w: w[1][1], ascending=False)\n",
    "#             )\n",
    "    rdd = (news_data.select(column, per_col)\n",
    "        .rdd\n",
    "        .flatMap(lambda row: [((row[per_col], w), 1) for w in row[column]])\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "        .map(lambda pair: (pair[0][0], (pair[0][1], pair[1])))\n",
    "        .groupByKey()\n",
    "        .map(lambda topic: (topic[0], sort(topic[1])))\n",
    "        .sortBy(lambda w: w[1][1], ascending=False)\n",
    "        )\n",
    "    if take == -1:\n",
    "        return rdd.collect()\n",
    "    else:\n",
    "        return rdd.take(take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:10.132239Z",
     "start_time": "2018-04-13T03:05:05.063617Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "task1_output.append('\\n[title top-frequent words per day]')\n",
    "for r in sorted(word_count_per(news_data, 'title_tokens', 'publishDate'), key=lambda x: x[0]):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:18.560854Z",
     "start_time": "2018-04-13T03:05:10.134542Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task1_output.append('\\n[headline top-frequent words per day]')\n",
    "for r in sorted(word_count_per(news_data, 'headline_tokens', 'publishDate'), key=lambda x: x[0]):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:21.792945Z",
     "start_time": "2018-04-13T03:05:18.563129Z"
    }
   },
   "outputs": [],
   "source": [
    "task1_output.append('\\n[title top-frequent words per topic]')\n",
    "for r in word_count_per(news_data, 'title_tokens', 'topic'):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:27.300043Z",
     "start_time": "2018-04-13T03:05:21.794798Z"
    }
   },
   "outputs": [],
   "source": [
    "task1_output.append('\\n[headline top-frequent words per topic]')\n",
    "for r in word_count_per(news_data, 'headline_tokens', 'topic'):\n",
    "    task1_output.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:27.336241Z",
     "start_time": "2018-04-13T03:05:27.302174Z"
    }
   },
   "outputs": [],
   "source": [
    "task1_file.writelines(['{}\\n'.format(r) for r in task1_output])\n",
    "task1_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In social feedback data, calculate the average popularity of each news by hour, and by day, respectively (for each platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:27.343988Z",
     "start_time": "2018-04-13T03:05:27.338804Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_social_data(data, files):\n",
    "    for f in files:\n",
    "        df = read_csv(f)\n",
    "        data = data.union(df) if data else df\n",
    "    for i in range(1, 144+1):\n",
    "        col_name = 'TS{}'.format(i)\n",
    "        data = data.withColumn(col_name, data[col_name].cast('int'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:27.350271Z",
     "start_time": "2018-04-13T03:05:27.346397Z"
    }
   },
   "outputs": [],
   "source": [
    "fb_social_data = google_social_data = linkedin_social_data = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:33.115352Z",
     "start_time": "2018-04-13T03:05:27.352701Z"
    }
   },
   "outputs": [],
   "source": [
    "fb_social_data = create_social_data(fb_social_data, files['fb'])\n",
    "google_social_data = create_social_data(google_social_data, files['google'])\n",
    "linkedin_social_data = create_social_data(linkedin_social_data, files['linkedin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:33.269119Z",
     "start_time": "2018-04-13T03:05:33.117428Z"
    }
   },
   "outputs": [],
   "source": [
    "fb_social_data = fb_social_data.dropna()\n",
    "google_social_data = google_social_data.dropna()\n",
    "linkedin_social_data = linkedin_social_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:33.274818Z",
     "start_time": "2018-04-13T03:05:33.271184Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_avg(seq):\n",
    "    sum_ = np.sum(seq)\n",
    "    return sum_/48, sum_/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:33.280617Z",
     "start_time": "2018-04-13T03:05:33.277059Z"
    }
   },
   "outputs": [],
   "source": [
    "def avg_popu(data, by=3):\n",
    "    return (data\n",
    "           .rdd\n",
    "           .map(lambda r: (r['IDLink'],  get_avg(r[1:])))\n",
    "           .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:38.913289Z",
     "start_time": "2018-04-13T03:05:33.282873Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fb_avg_by_hour_and_day = avg_popu(fb_social_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:43.254842Z",
     "start_time": "2018-04-13T03:05:38.915563Z"
    }
   },
   "outputs": [],
   "source": [
    "google_avg_by_hour_and_day = avg_popu(google_social_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:47.023554Z",
     "start_time": "2018-04-13T03:05:43.257122Z"
    }
   },
   "outputs": [],
   "source": [
    "linkedin_avg_by_hour_and_day = avg_popu(linkedin_social_data, by=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:47.029048Z",
     "start_time": "2018-04-13T03:05:47.025798Z"
    }
   },
   "outputs": [],
   "source": [
    "avg_popularity = {'fb':fb_avg_by_hour_and_day,\n",
    "                 'google':google_avg_by_hour_and_day,\n",
    "                 'linkedin':linkedin_avg_by_hour_and_day}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:47.036608Z",
     "start_time": "2018-04-13T03:05:47.031622Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_csv(file_name, data):\n",
    "    with open(file_name, 'w', \n",
    "              encoding='utf-8', newline='\\n') as csvfile:\n",
    "        fieldnames = ['IDLink', 'avg_popularity']\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        writer.writerow(fieldnames)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:47.672370Z",
     "start_time": "2018-04-13T03:05:47.038803Z"
    }
   },
   "outputs": [],
   "source": [
    "for platform, data in avg_popularity.items():\n",
    "    rows_by_hour = []\n",
    "    rows_by_day = []\n",
    "    \n",
    "    for ID, (avg_by_hour, avg_by_day) in data:\n",
    "        rows_by_hour.append((ID, avg_by_hour))\n",
    "        rows_by_day.append( (ID, avg_by_day))\n",
    "        \n",
    "    save_csv('./result/task2/{}_avg_popularity_by_hour.csv'.format(platform), \n",
    "             rows_by_hour)\n",
    "    save_csv('./result/task2/{}_avg_popularity_by_day.csv'.format(platform), \n",
    "             rows_by_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  In news data, calculate the sum and average sentiment score of each topic, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:47.679233Z",
     "start_time": "2018-04-13T03:05:47.674515Z"
    }
   },
   "outputs": [],
   "source": [
    "task3_file = open('result/task3/output.txt', 'w', encoding='utf-8', newline='\\n')\n",
    "task3_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:48.982074Z",
     "start_time": "2018-04-13T03:05:47.681184Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sum_of_score = (news_data.select('SentimentScore', 'topic')\n",
    "                .rdd\n",
    "                .map(lambda r: (r['topic'], (r['SentimentScore'], 1)))\n",
    "                .reduceByKey(lambda a, b: (a[0]+b[0], a[1]+b[1]))\n",
    "                .map(lambda x: (x[0], x[1][0], (x[1][0]/x[1][1])))\n",
    "#                 .groupByKey()\n",
    "#                 .map(lambda topic: (topic[0], \n",
    "#                                     np.sum(list(topic[1])), \n",
    "#                                     np.mean(list(topic[1]))))\n",
    "                .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:48.989344Z",
     "start_time": "2018-04-13T03:05:48.984505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "economy   , -1608.718, -0.050078\n",
      "microsoft , -268.917, -0.012572\n",
      "palestine , -509.273, -0.063311\n",
      "obama     , -498.149, -0.018407\n"
     ]
    }
   ],
   "source": [
    "task3_output.append('[sum sentiment score of each topic]')\n",
    "for topic, sum_, avg in sum_of_score:\n",
    "    print('{:10s}, {:.3f}, {:.6f}'.format(topic, sum_, avg))\n",
    "    task3_output.append('{:>10s}, {:.3f}'.format(topic, sum_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:48.995161Z",
     "start_time": "2018-04-13T03:05:48.991855Z"
    }
   },
   "outputs": [],
   "source": [
    "task3_output.append('[avg sentiment score of each topic]')\n",
    "for topic, sum_, avg in sum_of_score:\n",
    "    task3_output.append('{:>10s}, {:.6f}'.format(topic, avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:49.000318Z",
     "start_time": "2018-04-13T03:05:48.997172Z"
    }
   },
   "outputs": [],
   "source": [
    "task3_file.writelines(['{}\\n'.format(r) for r in task3_output])\n",
    "task3_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From subtask (1), for the top-100 frequent words per topic in titles and headlines, calculate their co-occurrence matrices (100x100), respectively. Each entry in the matrix will contain the co-occurrence frequency in all news titles and headlines, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:57.460668Z",
     "start_time": "2018-04-13T03:05:49.002235Z"
    }
   },
   "outputs": [],
   "source": [
    "fw_all = {'title_tokens':{topic:[w[0] for w in top] \n",
    "                          for topic, top in word_count_per(news_data, 'title_tokens', 'topic')}, \n",
    "         'headline_tokens':{topic:[w[0] for w in top] \n",
    "                          for topic, top in word_count_per(news_data, 'headline_tokens', 'topic')}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:05:57.466354Z",
     "start_time": "2018-04-13T03:05:57.462959Z"
    }
   },
   "outputs": [],
   "source": [
    "def counter(vocabulary, tokens):\n",
    "    return  [int(tokens.count(v) > 0) for v in vocabulary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:06:36.568235Z",
     "start_time": "2018-04-13T03:05:57.468399Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column name:title_tokens, topic:obama\n",
      "column name:title_tokens, topic:microsoft\n",
      "column name:title_tokens, topic:economy\n",
      "column name:title_tokens, topic:palestine\n",
      "column name:headline_tokens, topic:palestine\n",
      "column name:headline_tokens, topic:obama\n",
      "column name:headline_tokens, topic:microsoft\n",
      "column name:headline_tokens, topic:economy\n"
     ]
    }
   ],
   "source": [
    "for col_name, v in fw_all.items():\n",
    "    for topic, vocabulary in v.items():\n",
    "        print('column name:{}, topic:{}'.format(col_name, topic))\n",
    "        \n",
    "        X = np.array(news_data.select(col_name, 'topic')\n",
    "                     .rdd\n",
    "                     .filter(lambda r: r['topic'] == topic)\n",
    "                     .map(lambda r:counter(vocabulary, r[col_name]))\n",
    "                     .collect(), dtype='int64')\n",
    "        co_occ = X.T.dot(X)\n",
    "        # np.fill_diagonal(co_occ, 0)\n",
    "        df = pd.DataFrame(data=co_occ, columns=vocabulary, index=vocabulary)\n",
    "        # display(df)\n",
    "        df.to_csv('result/task4/{}_{}_matrix.csv'.format(col_name, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T03:06:36.575318Z",
     "start_time": "2018-04-13T03:06:36.570867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost 1.80 minutes\n"
     ]
    }
   ],
   "source": [
    "print('cost {:.2f} minutes'.format((time.time()-t0)/60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "171px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
